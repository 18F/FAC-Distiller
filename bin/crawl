#!/bin/bash

#
# This script is tailored to run on a cloud.gov sandbox account, which has
# resource restrictions that prevent concurrent jobs.
#
# NOTE: This is just a helper, and is not suitable for a real production
# environment.
#
# Sequentially run crawl jobs on cloud.gov, then load to the Distiller
# database.
#

set -euo pipefail

export AWS_ACCESS_KEY_ID=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.access_key_id"`
export AWS_SECRET_ACCESS_KEY=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.secret_access_key"`
export AWS_DEFAULT_REGION=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.region"`
export S3_BUCKET=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.bucket"`

# Put Scrapy metadata files in a timestamped directory.
UTC_DATE_NOW=`TZ=UTC date +%FT%TZ`
DIR_NAME=${1:-${UTC_DATE_NOW}}

# Crawl documents for each CFDA agency prefix from $START_YEAR to the present.
CUR_YEAR=`date +"%Y"`
START_YEAR=2013
#AGENCY_PREFIXES=`seq -w 1 0.1 99.9`
AGENCY_PREFIXES=`python manage.py print_agency_prefixes`
for AGENCY_PREFIX in ${AGENCY_PREFIXES}
do
    for YEAR in `seq ${START_YEAR} 1 ${CUR_YEAR}`
    do
        echo "Crawling CFDA prefix ${AGENCY_PREFIX} for year ${YEAR}"
        scrapy crawl fac --loglevel INFO -a cfda=${AGENCY_PREFIX} -a audit_year=${YEAR} -t csv -o "s3://${S3_BUCKET}/fac-crawls/${DIR_NAME}/${AGENCY_PREFIX}-${YEAR}.csv"
    done
done

# Load crawled documents into the database
echo "Loading crawled documents"
python manage.py load_fac_documents --reload --load-dir "${DIR_NAME}" --log

echo "Done loading documents!"
