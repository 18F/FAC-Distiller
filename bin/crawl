#!/bin/bash

#
# This script is tailored to run on a cloud.gov sandbox account, which has
# resource restrictions that prevent concurrent jobs.
#
# NOTE: This is just a helper, and is not suitable for a real production
# environment.
#
# Sequentially run crawl jobs on cloud.gov, then load to the Distiller
# database.
#

set -euo pipefail

export AWS_ACCESS_KEY_ID=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.access_key_id"`
export AWS_SECRET_ACCESS_KEY=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.secret_access_key"`
export AWS_DEFAULT_REGION=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.region"`
export S3_BUCKET=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.bucket"`

# Put Scrapy metadata files in a timestamped directory.
DIR_NAME=`TZ=UTC date`

# Crawl documents for each CFDA agency prefix.
# NOTE: Currently, this will limit the crawl to all 2019 documents.
for AGENCY_PREFIX in '01' '03' '04' '05' '06' '07' '08' '09' '10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '20' '21' '22' '23' '27' '29' '30' '32' '33' '34' '36' '39' '40' '42' '43' '44' '45' '46' '47' '53' '57' '58' '59' '60' '61' '62' '64' '66' '68' '70' '77' '78' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99'
do
    echo "Crawling CFDA prefix ${AGENCY_PREFIX}"
    scrapy crawl fac --loglevel INFO -a cfda=${AGENCY_PREFIX} -t csv -o "s3://${S3_BUCKET}/fac-crawls/${DIR_NAME}/${AGENCY_PREFIX}.csv"
done

# Load crawled documents into the database
echo "Loading crawled documents"
python manage.py load_fac_documents --reload --load-dir "${DIR_NAME}"

echo "Done loading documents!"
