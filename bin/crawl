#!/bin/bash

#
# This script is tailored to run on a cloud.gov sandbox account, which has
# resource restrictions that prevent concurrent jobs.
#
# NOTE: This is just a helper, and is not suitable for a real production
# environment.
#
# Sequentially run crawl jobs on cloud.gov, then load to the Distiller
# database.
#

set -euo pipefail

export AWS_ACCESS_KEY_ID=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.access_key_id"`
export AWS_SECRET_ACCESS_KEY=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.secret_access_key"`
export AWS_DEFAULT_REGION=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.region"`
export S3_BUCKET=`echo ${VCAP_SERVICES} | jq -r ".s3[0].credentials.bucket"`

# Put Scrapy metadata files in a timestamped directory.
UTC_DATE_NOW=`TZ=UTC date`
DIR_NAME=${1:-${UTC_DATE_NOW}}

# Crawl documents for each CFDA agency prefix.
# NOTE: Currently, this will limit the crawl to all 2018-2020 documents.
for AGENCY_PREFIX in `seq -w 1 0.1 99.9`
do
    echo "Crawling CFDA prefix ${AGENCY_PREFIX}"
    #scrapy crawl fac --loglevel INFO -a cfda=${AGENCY_PREFIX} -a audit_year=2018 -t csv -o "s3://${S3_BUCKET}/fac-crawls/${DIR_NAME}/${AGENCY_PREFIX}.csv"
    #scrapy crawl fac --loglevel INFO -a cfda=${AGENCY_PREFIX} -a audit_year=2019 -t csv -o "s3://${S3_BUCKET}/fac-crawls/${DIR_NAME}/${AGENCY_PREFIX}.csv"
    scrapy crawl fac --loglevel INFO -a cfda=${AGENCY_PREFIX} -a audit_year=2020 -t csv -o "s3://${S3_BUCKET}/fac-crawls/${DIR_NAME}/${AGENCY_PREFIX}.csv"
done

# Load crawled documents into the database
echo "Loading crawled documents"
python manage.py load_fac_documents --reload --load-dir "${DIR_NAME}"

echo "Done loading documents!"
